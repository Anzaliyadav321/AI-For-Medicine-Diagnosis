Now that you've seen how you would
go about training a model for medical diagnosis, let's talk about how
you would go about testing such a model. You will learn about the proper use of
training, validation, and test sets. And about the need for strong ground
truth in order to evaluate your models. When we apply machine
learning to a dataset, we usually split it into a training and
a test set. Our training set is used for the
development and selection of models and our test set for
the final reporting of our results. In reality, the training dataset is
further split into a training set and a validation set, where the training
set is used to learn a model and the validation set is used for
hyper parameter tuning and giving an estimate of the model
performance on the test set. Sometimes the split into a training and validation set is done multiple times
in a method called cross validation to reduce variability in our
estimate of the model performance. These sets also go by different names
sometimes like validation can be called tuning or depth set, the training set
can be called the development set, and the test set can go by holdout or
even more confusing the validation set. We will stick to the terms training,
validation, and test set for our purposes. We'll cover three challenges with building
these sets in the context of medicine. The first challenge relates to how
we make these test sets independent, the second relates to how we sample them,
and the third relates to how
we set the ground truth. Let's cover the problem
of patient overlap first. Let's say a patient comes in twice for an
x-ray, once in June and once in November. Both times, they're wearing a necklace
when they have their x-ray taken. One of their x-rays is sampled
as part of the training set and the other as part of test. We train our deep learning model and find that it correctly predicts normal for
the x-ray in the test set. The problem is that it's possible
that the model actually memorized to output normal when it saw
the patient with a necklace on. This is not hypothetical,
deep learning models can unintentionally memorize training
data, and the model could memorize rare or unique training data aspects of
the patient, such as the necklace, which could help it get the right answer
when testing on the same patient. This would lead to an overly
optimistic test set performance, where we would think that our model
is better than it actually is.